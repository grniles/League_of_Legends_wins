# but not a dramatic one
# read in libraries
library(MASS)
##### model assessment OUTER 10-fold CV (with model selection INNER 10-fold CV as part of model-fitting) #####
xy.out = league.data
n.out = dim(xy.out)[1]
#define the cross-validation splits
k.out = 10
groups.out = c(rep(1:k.out,floor(n.out/k.out)),1:(n.out%%k.out))  #produces list of group labels
set.seed(8, sample.kind = "Rounding")
cvgroups.out = sample(groups.out,n.out)  #orders randomly, with seed (8)
allpredictedCV.out = rep(NA,n.out)
##### model assessment OUTER shell #####
for (j in 1:k.out)  {  #be careful not to re-use loop indices
groupj.out = (cvgroups.out == j)
# define the training set for outer loop
trainxy.out = xy.out[!groupj.out,]
#define the validation set for outer loop
testxy.out = xy.out[groupj.out,]
##############################################
###   model selection on trainxy.out       ###
##############################################
##entire model-fitting process##
xy.in = trainxy.out  # fixed to be fit ONLY to the training data from the outer split
n.in = dim(xy.in)[1]
ncv = 10
if ((n.in%%ncv) == 0) {
groups.in= rep(1:ncv,floor(n.in/ncv))} else {
groups.in=c(rep(1:ncv,floor(n.in/ncv)),(1:(n.in%%ncv)))
}
cvgroups.in = sample(groups.in,n.in)
# set up storage
allpredictedcv10 = matrix(,ncol=6,nrow=n.in)
# with model selection
for (i in 1:ncv) {
newdata.in = xy.in[cvgroups.in==i,]
lda8fit = lda(blueWins ~ blueDragons + blueTotalGold + redEliteMonsters + redTotalGold + TowersDestroyed + AvgLevel + TotalMinionsKilled + TotalJungleMinionsKilled, data=xy.in, subset=(cvgroups.in!=i))
allpredictedcv10[cvgroups.in==i,1] = predict(lda8fit,newdata.in)$class
lda7fit = lda(blueWins ~ blueDragons + blueTotalGold + redEliteMonsters + redTotalGold + TowersDestroyed + AvgLevel + TotalJungleMinionsKilled, data=xy.in, subset=(cvgroups.in!=i))
allpredictedcv10[cvgroups.in==i,2] = predict(lda7fit,newdata.in)$class
lda5fit = lda(blueWins ~ blueDragons + blueTotalGold + redEliteMonsters + redTotalGold + AvgLevel, data=xy.in, subset=(cvgroups.in!=i))
allpredictedcv10[cvgroups.in==i,3] = predict(lda5fit,newdata.in)$class
lda6fit = lda(blueWins ~ blueDragons + blueTotalGold + redEliteMonsters + redTotalGold + TowersDestroyed + AvgLevel, data=xy.in, subset=(cvgroups.in!=i))
allpredictedcv10[cvgroups.in==i,4] = predict(lda6fit,newdata.in)$class
}
#relabel as original values, not factor levels
allpredictedcv10 = allpredictedcv10-1  # now a table of predicted 0-1 values for HD
#compute the CV values
allcv10 = rep(0,4)
for (m in 1:4) allcv10[m] = sum(xy.in$blueWins!=allpredictedcv10[,m])/n.in
bestmodels = (1:4)[allcv10 == min(allcv10)]
##############################################
###   resulting in bestmodels              ###
##############################################
bestmodel = ifelse(length(bestmodels)==1,bestmodels,sample(bestmodels,1))
print(allcv10)
print(paste("Best model at outer loop",j,"is",bestmodel))
#some code-checking assistance:
#print(j)
#print(allcv10.in)
#print(bestmodels)
#print(bestmodel)
if (bestmodel == 1)  {
lda8fit.train = lda(blueWins ~ blueDragons + blueTotalGold + redEliteMonsters + redTotalGold + TowersDestroyed + AvgLevel + TotalMinionsKilled + TotalJungleMinionsKilled, data=trainxy.out)
predictvalid = as.numeric(predict(lda8fit.train, testxy.out)$class)
}
if (bestmodel == 2)  {
lda7fit.train = lda(blueWins ~ blueDragons + blueTotalGold + redEliteMonsters + redTotalGold + TowersDestroyed + AvgLevel + TotalJungleMinionsKilled, data=trainxy.out)
predictvalid = as.numeric(predict(lda7fit.train, testxy.out)$class)
}
if (bestmodel == 3)  {
lda5fit.train = lda(blueWins ~ blueDragons + blueTotalGold + redEliteMonsters + redTotalGold + AvgLevel, data=trainxy.out)
predictvalid = as.numeric(predict(lda5fit.train, testxy.out)$class)
}
if (bestmodel == 4)  {
lda6fit.train = lda(blueWins ~ blueDragons + blueTotalGold + redEliteMonsters + redTotalGold + TowersDestroyed + AvgLevel, data=trainxy.out)
predictvalid = as.numeric(predict(lda6fit.train, testxy.out)$class)
}
#relabel as original values, not factor levels
predictvalid = predictvalid-1  # now a vector of predicted 0-1 values for HD in validation set
allpredictedCV.out[groupj.out] = predictvalid
}
# the output shows the different models selected in the outer loop - purpose is only to observe processing
# however, the model selection was done previously (in Problem 4) via single-level cross-validation
#Purpose of double cross-validation:
# assessment - what proportion of the cross-validated classifications (valid predictions of
# new observations, based on model selected using the entire model-selection process)
# match the actual observations?
table(league.data$blueWins,allpredictedCV.out)
CV10.out = sum(league.data$blueWins!=allpredictedCV.out)/n.out
p.out = 1-CV10.out; p.out  # (133+89)/303
# this sounds pretty reasonable; but note that just always GUESSING the majority
# classification, 0, would result in a proportion correctly classified of 0.541...
table(league.data$blueWins)/n.out
# so (cross-validated) proportion 0.733 of correct classifications  is an improvement,
# but not a dramatic one
?tune
set.seed(9, sample.kind="Rounding")
tune.out = tune(svm, blueWins ~ ., data = league.data, kernel = 'linear', type = 'C-classification', ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
best.mod = tune.out$best.model
summary(best.mod)
summary(tune.out)
library(e1071)
set.seed(9, sample.kind="Rounding")
tune.out = tune(svm, blueWins ~ ., data = league.data, kernel = 'linear', type = 'C-classification', ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
best.mod = tune.out$best.model
summary(best.mod)
summary(tune.out)
set.seed(9, sample.kind="Rounding")
tune.out = tune(svm, blueWins ~ ., data = league.data, kernel = 'linear', type = 'C-classification', ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
best.mod = tune.out$best.model
summary(best.mod)
summary(tune.out)
set.seed(9, sample.kind="Rounding")
tune.out = tune(svm, blueWins ~ ., data = league.data, type = 'C-classification', ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
best.mod = tune.out$best.model
summary(best.mod)
summary(tune.out)
tune.out = tune(svm, blueWins ~ ., data = league.data, type = 'C-classification', ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
tune.out = tune(svm, factor(blueWins) ~ ., data = league.data, type = 'C-classification', ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
best.mod = tune.out$best.model
summary(best.mod)
summary(tune.out)
league = read.csv("high_diamond_ranked_10min.csv")
summary(league)
plots(league)
library(leaps)
league[1] = NULL
league.data = league
league.data$WardsPlaced = league.data$blueWardsPlaced - league.data$redWardsPlaced
league.data$blueWardsPlaced = NULL
league.data$redWardsPlaced = NULL
hist(league.data$WardsPlaced)
league.data$WardsDestroyed = league.data$blueWardsDestroyed - league.data$redWardsDestroyed
league.data$blueWardsDestroyed = NULL
league.data$redWardsDestroyed = NULL
hist(league.data$WardsDestroyed)
league.data$blueFirstBlood = as.factor(league.data$blueFirstBlood)
league.data$redFirstBlood = NULL
league.data$Kills = league$blueKills - league$redKills
league.data$blueKills = NULL
league.data$redKills = NULL
hist(league.data$Kills)
# Deaths and Kills are inversely related, removing deaths and focusing on kills
league.data$blueDeaths = NULL
league.data$redDeaths = NULL
hist(league.data$Deaths)
#league.data$Assists = league.data$blueAssists - league.data$redAssists
league.data$blueAssists = NULL
league.data$redAssists = NULL
hist(league.data$Assists)
#league.data$EliteMonsters = league.data$blueEliteMonsters - league.data$redEliteMonsters
#league.data$blueEliteMonsters = NULL
#league.data$redEliteMonsters = NULL
#league.data$Dragons = league.data$blueDragons - league.data$redDragons
#league.data$blueDragons = NULL
#league.data$redDragons = NULL
#league.data$Heralds = league.data$blueHeralds - league.data$redHeralds
league.data$blueHeralds = NULL
league.data$redHeralds = NULL
league.data$TowersDestroyed = league.data$blueTowersDestroyed - league.data$redTowersDestroyed
league.data$blueTowersDestroyed = NULL
league.data$redTowersDestroyed = NULL
hist(league.data$TowersDestroyed)
#league.data$TotalGold = league.data$blueTotalGold - league.data$redTotalGold
#league.data$blueTotalGold = NULL
#league.data$redTotalGold = NULL
#hist(league.data$TotalGold)
league.data$AvgLevel = league.data$blueAvgLevel - league.data$redAvgLevel
league.data$blueAvgLevel = NULL
league.data$redAvgLevel = NULL
hist(league.data$AvgLevel)
# Level and Experience are related, Levels rely on experience and are more significant indicators of strength than experience
league.data$blueTotalExperience = NULL
league.data$redTotalExperience = NULL
hist(league.data$TotalExperience)
league.data$TotalMinionsKilled = league.data$blueTotalMinionsKilled - league.data$redTotalMinionsKilled
league.data$blueTotalMinionsKilled = NULL
league.data$redTotalMinionsKilled = NULL
hist(league.data$TotalMinionsKilled)
league.data$TotalJungleMinionsKilled = league.data$blueTotalJungleMinionsKilled - league.data$redTotalJungleMinionsKilled
league.data$blueTotalJungleMinionsKilled = NULL
league.data$redTotalJungleMinionsKilled = NULL
hist(league.data$TotalJungleMinionsKilled)
#golddiff and TotalGold highly correlated
league.data$blueGoldDiff = NULL
league.data$redGoldDiff = NULL
# ExperienceDiff and TotalExperience high correlated
league.data$blueExperienceDiff = NULL
league.data$redExperienceDiff = NULL
# CSPerMin high correlated with totalMinionsKilled
league.data$blueCSPerMin = NULL
league.data$redCSPerMin = NULL
# Gold/min and totalGold are strongly correlated, removing goldpermin variables
league.data$blueGoldPerMin = NULL
league.data$redGoldPerMin = NULL
summary(league.data)
fullmod = glm(blueWins ~ ., data=league.data, family = "binomial")
nothing = glm(blueWins ~ 1, data=league.data, family = "binomial")
backwards = step(fullmod)
summary(backwards)
forwards = step(nothing, scope=list(lower=formula(nothing),upper=formula(fullmod), direction="forward"))
summary(forwards)
summary(backwards)
library(car)
fit = glm(blueWins~., data=league.data, family='binomial')
vif(fit)
# read in libraries
library(MASS)
##### model assessment OUTER 10-fold CV (with model selection INNER 10-fold CV as part of model-fitting) #####
xy.out = league.data
n.out = dim(xy.out)[1]
#define the cross-validation splits
k.out = 10
groups.out = c(rep(1:k.out,floor(n.out/k.out)),1:(n.out%%k.out))  #produces list of group labels
set.seed(8, sample.kind = "Rounding")
cvgroups.out = sample(groups.out,n.out)  #orders randomly, with seed (8)
allpredictedCV.out = rep(NA,n.out)
##### model assessment OUTER shell #####
for (j in 1:k.out)  {  #be careful not to re-use loop indices
groupj.out = (cvgroups.out == j)
# define the training set for outer loop
trainxy.out = xy.out[!groupj.out,]
#define the validation set for outer loop
testxy.out = xy.out[groupj.out,]
##############################################
###   model selection on trainxy.out       ###
##############################################
##entire model-fitting process##
xy.in = trainxy.out  # fixed to be fit ONLY to the training data from the outer split
n.in = dim(xy.in)[1]
ncv = 10
if ((n.in%%ncv) == 0) {
groups.in= rep(1:ncv,floor(n.in/ncv))} else {
groups.in=c(rep(1:ncv,floor(n.in/ncv)),(1:(n.in%%ncv)))
}
cvgroups.in = sample(groups.in,n.in)
# set up storage
allpredictedcv10 = matrix(,ncol=6,nrow=n.in)
# with model selection
for (i in 1:ncv) {
newdata.in = xy.in[cvgroups.in==i,]
lda8fit = lda(blueWins ~ blueDragons + blueTotalGold + redEliteMonsters + redTotalGold + TowersDestroyed + AvgLevel + TotalMinionsKilled + TotalJungleMinionsKilled, data=xy.in, subset=(cvgroups.in!=i))
allpredictedcv10[cvgroups.in==i,1] = predict(lda8fit,newdata.in)$class
lda7fit = lda(blueWins ~ blueDragons + blueTotalGold + redEliteMonsters + redTotalGold + TowersDestroyed + AvgLevel + TotalJungleMinionsKilled, data=xy.in, subset=(cvgroups.in!=i))
allpredictedcv10[cvgroups.in==i,2] = predict(lda7fit,newdata.in)$class
lda5fit = lda(blueWins ~ blueDragons + blueTotalGold + redEliteMonsters + redTotalGold + AvgLevel, data=xy.in, subset=(cvgroups.in!=i))
allpredictedcv10[cvgroups.in==i,3] = predict(lda5fit,newdata.in)$class
lda6fit = lda(blueWins ~ blueDragons + blueTotalGold + redEliteMonsters + redTotalGold + TowersDestroyed + AvgLevel, data=xy.in, subset=(cvgroups.in!=i))
allpredictedcv10[cvgroups.in==i,4] = predict(lda6fit,newdata.in)$class
}
#relabel as original values, not factor levels
allpredictedcv10 = allpredictedcv10-1  # now a table of predicted 0-1 values for HD
#compute the CV values
allcv10 = rep(0,4)
for (m in 1:4) allcv10[m] = sum(xy.in$blueWins!=allpredictedcv10[,m])/n.in
bestmodels = (1:4)[allcv10 == min(allcv10)]
##############################################
###   resulting in bestmodels              ###
##############################################
bestmodel = ifelse(length(bestmodels)==1,bestmodels,sample(bestmodels,1))
print(allcv10)
print(paste("Best model at outer loop",j,"is",bestmodel))
#some code-checking assistance:
#print(j)
#print(allcv10.in)
#print(bestmodels)
#print(bestmodel)
if (bestmodel == 1)  {
lda8fit.train = lda(blueWins ~ blueDragons + blueTotalGold + redEliteMonsters + redTotalGold + TowersDestroyed + AvgLevel + TotalMinionsKilled + TotalJungleMinionsKilled, data=trainxy.out)
predictvalid = as.numeric(predict(lda8fit.train, testxy.out)$class)
}
if (bestmodel == 2)  {
lda7fit.train = lda(blueWins ~ blueDragons + blueTotalGold + redEliteMonsters + redTotalGold + TowersDestroyed + AvgLevel + TotalJungleMinionsKilled, data=trainxy.out)
predictvalid = as.numeric(predict(lda7fit.train, testxy.out)$class)
}
if (bestmodel == 3)  {
lda5fit.train = lda(blueWins ~ blueDragons + blueTotalGold + redEliteMonsters + redTotalGold + AvgLevel, data=trainxy.out)
predictvalid = as.numeric(predict(lda5fit.train, testxy.out)$class)
}
if (bestmodel == 4)  {
lda6fit.train = lda(blueWins ~ blueDragons + blueTotalGold + redEliteMonsters + redTotalGold + TowersDestroyed + AvgLevel, data=trainxy.out)
predictvalid = as.numeric(predict(lda6fit.train, testxy.out)$class)
}
#relabel as original values, not factor levels
predictvalid = predictvalid-1  # now a vector of predicted 0-1 values for HD in validation set
allpredictedCV.out[groupj.out] = predictvalid
}
# the output shows the different models selected in the outer loop - purpose is only to observe processing
# however, the model selection was done previously (in Problem 4) via single-level cross-validation
#Purpose of double cross-validation:
# assessment - what proportion of the cross-validated classifications (valid predictions of
# new observations, based on model selected using the entire model-selection process)
# match the actual observations?
table(league.data$blueWins,allpredictedCV.out)
CV10.out = sum(league.data$blueWins!=allpredictedCV.out)/n.out
p.out = 1-CV10.out; p.out  # (133+89)/303
# this sounds pretty reasonable; but note that just always GUESSING the majority
# classification, 0, would result in a proportion correctly classified of 0.541...
table(league.data$blueWins)/n.out
# so (cross-validated) proportion 0.733 of correct classifications  is an improvement,
# but not a dramatic one
library(e1071)
set.seed(9, sample.kind="Rounding")
tune.out = tune(svm, factor(blueWins) ~ ., data = league.data, type = 'C-classification', ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
best.mod = tune.out$best.model
summary(best.mod)
summary(tune.out)
library(e1071)
set.seed(9, sample.kind="Rounding")
tune.out = tune(svm, factor(blueWins) ~ ., data = league.data, type = 'C-classification', ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100), gamma = c(0.5, 1, 2, 3, 4)))
best.mod = tune.out$best.model
summary(best.mod)
summary(tune.out)
plot(tune.out)
plot(best.mod)
plot(p.out)
best.mod$classes
best.mod$decision.values
best.mod$terms
best.mod$fitted
summary(best.mod)
summary(tune.out)
backwards = step(fullmod)
fit = glm(blueWins~., data=league.data, family='binomial')
vif(fit)
p.out = 1-CV10.out; p.out  # (133+89)/303
#Purpose of double cross-validation:
# assessment - what proportion of the cross-validated classifications (valid predictions of
# new observations, based on model selected using the entire model-selection process)
# match the actual observations?
table(league.data$blueWins,allpredictedCV.out)
CV10.out = sum(league.data$blueWins!=allpredictedCV.out)/n.out
p.out = 1-CV10.out; p.out  # (133+89)/303
#Purpose of double cross-validation:
# assessment - what proportion of the cross-validated classifications (valid predictions of
# new observations, based on model selected using the entire model-selection process)
# match the actual observations?
table(league.data$blueWins,allpredictedCV.out)
##### model assessment OUTER shell #####
for (j in 1:k.out)  {  #be careful not to re-use loop indices
groupj.out = (cvgroups.out == j)
# define the training set for outer loop
trainxy.out = xy.out[!groupj.out,]
#define the validation set for outer loop
testxy.out = xy.out[groupj.out,]
##############################################
###   model selection on trainxy.out       ###
##############################################
##entire model-fitting process##
xy.in = trainxy.out  # fixed to be fit ONLY to the training data from the outer split
n.in = dim(xy.in)[1]
ncv = 10
if ((n.in%%ncv) == 0) {
groups.in= rep(1:ncv,floor(n.in/ncv))} else {
groups.in=c(rep(1:ncv,floor(n.in/ncv)),(1:(n.in%%ncv)))
}
cvgroups.in = sample(groups.in,n.in)
# set up storage
allpredictedcv10 = matrix(,ncol=6,nrow=n.in)
# with model selection
for (i in 1:ncv) {
newdata.in = xy.in[cvgroups.in==i,]
lda8fit = lda(blueWins ~ blueDragons + blueTotalGold + redEliteMonsters + redTotalGold + TowersDestroyed + AvgLevel + TotalMinionsKilled + TotalJungleMinionsKilled, data=xy.in, subset=(cvgroups.in!=i))
allpredictedcv10[cvgroups.in==i,1] = predict(lda8fit,newdata.in)$class
lda7fit = lda(blueWins ~ blueDragons + blueTotalGold + redEliteMonsters + redTotalGold + TowersDestroyed + AvgLevel + TotalJungleMinionsKilled, data=xy.in, subset=(cvgroups.in!=i))
allpredictedcv10[cvgroups.in==i,2] = predict(lda7fit,newdata.in)$class
lda5fit = lda(blueWins ~ blueDragons + blueTotalGold + redEliteMonsters + redTotalGold + AvgLevel, data=xy.in, subset=(cvgroups.in!=i))
allpredictedcv10[cvgroups.in==i,3] = predict(lda5fit,newdata.in)$class
lda6fit = lda(blueWins ~ blueDragons + blueTotalGold + redEliteMonsters + redTotalGold + TowersDestroyed + AvgLevel, data=xy.in, subset=(cvgroups.in!=i))
allpredictedcv10[cvgroups.in==i,4] = predict(lda6fit,newdata.in)$class
}
#relabel as original values, not factor levels
allpredictedcv10 = allpredictedcv10-1  # now a table of predicted 0-1 values for HD
#compute the CV values
allcv10 = rep(0,4)
for (m in 1:4) allcv10[m] = sum(xy.in$blueWins!=allpredictedcv10[,m])/n.in
bestmodels = (1:4)[allcv10 == min(allcv10)]
##############################################
###   resulting in bestmodels              ###
##############################################
bestmodel = ifelse(length(bestmodels)==1,bestmodels,sample(bestmodels,1))
print(allcv10)
print(paste("Best model at outer loop",j,"is",bestmodel))
#some code-checking assistance:
#print(j)
#print(allcv10.in)
#print(bestmodels)
#print(bestmodel)
if (bestmodel == 1)  {
lda8fit.train = lda(blueWins ~ blueDragons + blueTotalGold + redEliteMonsters + redTotalGold + TowersDestroyed + AvgLevel + TotalMinionsKilled + TotalJungleMinionsKilled, data=trainxy.out)
predictvalid = as.numeric(predict(lda8fit.train, testxy.out)$class)
}
if (bestmodel == 2)  {
lda7fit.train = lda(blueWins ~ blueDragons + blueTotalGold + redEliteMonsters + redTotalGold + TowersDestroyed + AvgLevel + TotalJungleMinionsKilled, data=trainxy.out)
predictvalid = as.numeric(predict(lda7fit.train, testxy.out)$class)
}
if (bestmodel == 3)  {
lda5fit.train = lda(blueWins ~ blueDragons + blueTotalGold + redEliteMonsters + redTotalGold + AvgLevel, data=trainxy.out)
predictvalid = as.numeric(predict(lda5fit.train, testxy.out)$class)
}
if (bestmodel == 4)  {
lda6fit.train = lda(blueWins ~ blueDragons + blueTotalGold + redEliteMonsters + redTotalGold + TowersDestroyed + AvgLevel, data=trainxy.out)
predictvalid = as.numeric(predict(lda6fit.train, testxy.out)$class)
}
#relabel as original values, not factor levels
predictvalid = predictvalid-1  # now a vector of predicted 0-1 values for HD in validation set
allpredictedCV.out[groupj.out] = predictvalid
}
bestmodel
# this sounds pretty reasonable; but note that just always GUESSING the majority
# classification, 0, would result in a proportion correctly classified of 0.541...
table(league.data$blueWins)/n.out
p.out = 1-CV10.out; p.out  # (133+89)/303
# this sounds pretty reasonable; but note that just always GUESSING the majority
# classification, 0, would result in a proportion correctly classified of 0.541...
table(league.data$blueWins)/n.out
# this sounds pretty reasonable; but note that just always GUESSING the majority
# classification, 0, would result in a proportion correctly classified of 0.541...
table(league.data$blueWins)/n.out
best.mod = tune.out$best.model
summary(best.mod)
summary(tune.out)
best.mod$fitted
summary(best.mod)
summary(tune.out)
##### model assessment OUTER shell #####
for (j in 1:k.out)  {  #be careful not to re-use loop indices
groupj.out = (cvgroups.out == j)
# define the training set for outer loop
trainxy.out = xy.out[!groupj.out,]
#define the validation set for outer loop
testxy.out = xy.out[groupj.out,]
##############################################
###   model selection on trainxy.out       ###
##############################################
##entire model-fitting process##
xy.in = trainxy.out  # fixed to be fit ONLY to the training data from the outer split
n.in = dim(xy.in)[1]
ncv = 10
if ((n.in%%ncv) == 0) {
groups.in= rep(1:ncv,floor(n.in/ncv))} else {
groups.in=c(rep(1:ncv,floor(n.in/ncv)),(1:(n.in%%ncv)))
}
cvgroups.in = sample(groups.in,n.in)
# set up storage
allpredictedcv10 = matrix(,ncol=6,nrow=n.in)
# with model selection
for (i in 1:ncv) {
newdata.in = xy.in[cvgroups.in==i,]
lda8fit = lda(blueWins ~ blueDragons + blueTotalGold + redEliteMonsters + redTotalGold + TowersDestroyed + AvgLevel + TotalMinionsKilled + TotalJungleMinionsKilled, data=xy.in, subset=(cvgroups.in!=i))
allpredictedcv10[cvgroups.in==i,1] = predict(lda8fit,newdata.in)$class
lda7fit = lda(blueWins ~ blueDragons + blueTotalGold + redEliteMonsters + redTotalGold + TowersDestroyed + AvgLevel + TotalJungleMinionsKilled, data=xy.in, subset=(cvgroups.in!=i))
allpredictedcv10[cvgroups.in==i,2] = predict(lda7fit,newdata.in)$class
lda5fit = lda(blueWins ~ blueDragons + blueTotalGold + redEliteMonsters + redTotalGold + AvgLevel, data=xy.in, subset=(cvgroups.in!=i))
allpredictedcv10[cvgroups.in==i,3] = predict(lda5fit,newdata.in)$class
lda6fit = lda(blueWins ~ blueDragons + blueTotalGold + redEliteMonsters + redTotalGold + TowersDestroyed + AvgLevel, data=xy.in, subset=(cvgroups.in!=i))
allpredictedcv10[cvgroups.in==i,4] = predict(lda6fit,newdata.in)$class
}
#relabel as original values, not factor levels
allpredictedcv10 = allpredictedcv10-1  # now a table of predicted 0-1 values for HD
#compute the CV values
allcv10 = rep(0,4)
for (m in 1:4) allcv10[m] = sum(xy.in$blueWins!=allpredictedcv10[,m])/n.in
bestmodels = (1:4)[allcv10 == min(allcv10)]
##############################################
###   resulting in bestmodels              ###
##############################################
bestmodel = ifelse(length(bestmodels)==1,bestmodels,sample(bestmodels,1))
print(allcv10)
print(paste("Best model at outer loop",j,"is",bestmodel))
#some code-checking assistance:
#print(j)
#print(allcv10.in)
#print(bestmodels)
#print(bestmodel)
if (bestmodel == 1)  {
lda8fit.train = lda(blueWins ~ blueDragons + blueTotalGold + redEliteMonsters + redTotalGold + TowersDestroyed + AvgLevel + TotalMinionsKilled + TotalJungleMinionsKilled, data=trainxy.out)
predictvalid = as.numeric(predict(lda8fit.train, testxy.out)$class)
}
if (bestmodel == 2)  {
lda7fit.train = lda(blueWins ~ blueDragons + blueTotalGold + redEliteMonsters + redTotalGold + TowersDestroyed + AvgLevel + TotalJungleMinionsKilled, data=trainxy.out)
predictvalid = as.numeric(predict(lda7fit.train, testxy.out)$class)
}
if (bestmodel == 3)  {
lda5fit.train = lda(blueWins ~ blueDragons + blueTotalGold + redEliteMonsters + redTotalGold + AvgLevel, data=trainxy.out)
predictvalid = as.numeric(predict(lda5fit.train, testxy.out)$class)
}
if (bestmodel == 4)  {
lda6fit.train = lda(blueWins ~ blueDragons + blueTotalGold + redEliteMonsters + redTotalGold + TowersDestroyed + AvgLevel, data=trainxy.out)
predictvalid = as.numeric(predict(lda6fit.train, testxy.out)$class)
}
#relabel as original values, not factor levels
predictvalid = predictvalid-1  # now a vector of predicted 0-1 values for HD in validation set
allpredictedCV.out[groupj.out] = predictvalid
}
# the output shows the different models selected in the outer loop - purpose is only to observe processing
# however, the model selection was done previously (in Problem 4) via single-level cross-validation
#Purpose of double cross-validation:
# assessment - what proportion of the cross-validated classifications (valid predictions of
# new observations, based on model selected using the entire model-selection process)
# match the actual observations?
table(league.data$blueWins,allpredictedCV.out)
CV10.out = sum(league.data$blueWins!=allpredictedCV.out)/n.out
p.out = 1-CV10.out; p.out  # (133+89)/303
# this sounds pretty reasonable; but note that just always GUESSING the majority
# classification, 0, would result in a proportion correctly classified of 0.541...
table(league.data$blueWins)/n.out
# so (cross-validated) proportion 0.733 of correct classifications  is an improvement,
# but not a dramatic one
source('C:/Users/niles/OneDrive/Desktop/MSDS/DS740/Final/Final.R', echo=TRUE)
source('C:/Users/niles/OneDrive/Desktop/MSDS/DS740/Final/Final.R', echo=TRUE)
